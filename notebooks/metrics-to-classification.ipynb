{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ['cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', \n",
    "     'dog', 'dog', 'dog', 'dog', 'dog', 'dog', \n",
    "     'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', \n",
    "     'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ['cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'dog', 'dog', \n",
    "          'cat', 'cat', 'dog', 'dog', 'dog', 'rabbit',\n",
    "          'dog', 'dog', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', \n",
    "         'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit',  'rabbit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  3,  0],\n",
       "       [ 2,  3,  1],\n",
       "       [ 0,  2, 11]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  3,  0],\n",
       "       [ 2,  3,  1],\n",
       "       [ 0,  2, 11]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7037037037037037"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71428571 0.375      0.91666667]\n",
      "0.6686507936507936\n"
     ]
    }
   ],
   "source": [
    "precision = metrics.precision_score(y, y_pred, average=None)\n",
    "precision.T\n",
    "precision_mean = np.mean(precision)\n",
    "print(precision)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.625      0.5        0.84615385]\n",
      "0.657051282051282\n"
     ]
    }
   ],
   "source": [
    "recall = metrics.recall_score(y, y_pred, average=None)\n",
    "recall.T\n",
    "recall_mean = np.mean(recall)\n",
    "print(recall)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 0.42857143 0.88      ]\n",
      "0.6584126984126983\n"
     ]
    }
   ],
   "source": [
    "f1 = metrics.f1_score(y, y_pred, average=None)\n",
    "f1.T\n",
    "f1_mean = np.mean(f1)\n",
    "print(f1)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.71      0.62      0.67         8\n",
      "         dog       0.38      0.50      0.43         6\n",
      "      rabbit       0.92      0.85      0.88        13\n",
      "\n",
      "   micro avg       0.70      0.70      0.70        27\n",
      "   macro avg       0.67      0.66      0.66        27\n",
      "weighted avg       0.74      0.70      0.72        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve = metrics.roc_curve(y_true, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss = metrics.log_loss(y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
